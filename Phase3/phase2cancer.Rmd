---
title: "phase2cancer"
output: html_notebook
date: "2023-10-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Problem

The problem is the accurate diagnosis of breast masses as malignant (cancerous) or benign (non- cancerous) using computed features from digitized images of fine needle aspirate (FNA). The features consist of radius, texture, perimeter, area, smoothness, compactness, concavity, concave.points, symmetry, and fractal_dimension of the mass,and there mean, worst, and standard error(SE).

The goal is to predict the nature of breast tumors based on the analyzed features, enabling medical professionals to make informed decisions regarding patient treatment and care.

Solving the problem of diagnosing breast cancer is important because it can lead to early detection, appropriate treatment plans, improved survival rates, and optimal allocation of healthcare resources.

# 2. Data Mining Task

The problem can be defined as a data mining binary classification task. The task involves training models and algorithms on the features of breast masses to predict whether a specific mass is malignant(M) or benign(B). The objective is to optimize the classification model's accuracy and performance in differentiating between the two classes using the dataset as the training data. Data mining techniques such as feature selection, model training, and performance evaluation would be employed to solve this task efficiently.

Additionally, there is a potential clustering dimension to the task. Clustering involves grouping similar instances together based on inherent patterns or similarities in the data. In this case, clustering techniques could be applied to identify natural groupings or clusters within the dataset, which might reveal hidden structures or relationships among the breast masses. This could aid in gaining a deeper understanding of the underlying characteristics of different masses.

# 3. Data

We will go over a detailed description of the dataset used in this project. The dataset in focus is the Breast Cancer Wisconsin (Diagnostic) dataset, which plays a crucial role in achieving the goal of accurate breast cancer diagnosis and classification.

### The source of the dataset

<https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data>

### General information about the dataset

1- Number of attributes (variables, or columns): 32 (the last column (X) Contains only missing "NA" values, that why we didn't count it, and it well be removed later on)

2- Number of objects (records, or rows): The dataset contains a total of 569 instances, each representing a sample of a breast mass.

3- all variables are numeric, except id, and diagnosis.

4- The ID number is a nominal attribute

5- diagnosis is a binary (categoric, or factor) attribute and has two levels of values "B","M".

6- Class name or labels: The diagnosis attribute represents the class labels, with "M" indicating malignant (cancerous) tumors and "B" indicating benign (non-cancerous) tumors.

7- Class distribution: The dataset is imbalanced, with 357 instances classified as benign and 212 instances classified as malignant.

### Attributes table

The attributes table present important information about the attributes associated with the dataset. This table serves as a reference guide, outlining the different attributes, their description, data type, and possible values. It aims to provide a comprehensive overview of the dataset, enabling us to understand and utilize the information effectively.

| No  | Attributes              | Description                                                                                                                                                      | Data type                      | Possible values                  |
|-----|-------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------|----------------------------------|
| 1   | id                      | Id number for applicant                                                                                                                                          | nominal                        | Range between 8670-911320502     |
| 2   | diagnosis               | indicate whether a case is classified as malignant (cancerous) or benign.                                                                                        | (Asymmetric Binary)categorical | "M" for malignant "B" for benign |
| 3   | radius_mean             | Radius: is a measurement of the average distance from the center of the nucleus to its boundary. Mean: mean of distances from center to points on the perimeter. | numerical                      | Range between 6.9-28.1           |
| 4   | texture_mean            | standard deviation of gray-scale values.                                                                                                                         | numerical                      | Range between 9.7-39.2           |
| 5   | perimeter_mean          | Perimeter: total length of the boundary of the nucleus. Mean: mean size of the core tumor                                                                        | numerical                      | Range between 43.7-188.5         |
| 6   | area_mean               | The mean value of total area occupied by the nucleus.                                                                                                            | numerical                      | Range between 143.5-782.7        |
| 7   | smoothness_mean         | local variation in the radius lengths of the cell nuclei present in a breast mass. It quantifies the smoothness of the boundaries of the nuclei.                 | numerical                      | Range between 0.05-o.16          |
| 8   | compactness_mean        | measures the smoothness of the boundaries and relates to the compactness of the shape of the nuclei. mean of perimeter\^2/ area - 1.0                            | numerical                      | Range between 0.01-0.34          |
| 9   | concavity_mean          | mean of severity of concave portions of the contour                                                                                                              | numerical                      | Range between 0.0-0.4            |
| 10  | concave.points_mean     | mean for number of concave portions of the contour                                                                                                               | numerical                      | Range between 0.0-0.2            |
| 11  | symmetry_mean           | Mean quantifies the degree to which the shape of the nuclei is symmetrical.                                                                                      | numerical                      | Range between 0.1-0.3            |
| 12  | fractal_dimension_mean  | quantifies the complexity and irregularity of the nuclei's shape using fractal geometry. mean for "coastline approximation" - 1                                  | numerical                      | Range between 0.04-0.09          |
| 13  | radius_se               | standard error for the mean of distances from center to points on the perimeter                                                                                  | numerical                      | Range between 0.1-2.8            |
| 14  | texture_se              | standard error for standard deviation of gray-scale values                                                                                                       | numerical                      | Range between 0.3-4.8            |
| 15  | perimeter_se            | Standard error for mean size of the core tumor                                                                                                                   | numerical                      | Range between 0.7-21.9           |
| 16  | area_se                 | Standard error for The mean value of total area occupied by the nucleus.                                                                                         | numerical                      | Range between 6.8-542.2          |
| 17  | smoothness_se           | standard error for local variation in radius lengths                                                                                                             | numerical                      | Range between 0.001-0.0311       |
| 18  | compactness_se          | standard error for perimeter\^2 / area -1.0                                                                                                                      | numerical                      | Range between 0.002-0.135        |
| 19  | concavity_se            | standard error for severity of concave portions of the contour                                                                                                   | numerical                      | Range between 0.00-0.39          |
| 20  | concave.points_se       | standard error for number of concave portions of the contour                                                                                                     | numerical                      | Range between 0.00-0.05          |
| 21  | symmetry_se             | Standard error for the mean that quantifies the degree to which the shape of the nuclei is symmetrical.                                                          | numerical                      | Range between 0.007-0.078        |
| 22  | fractal_dimension_se    | standard error for "coastline approximation" - 1                                                                                                                 | numerical                      | Range between 0.000-0.029        |
| 23  | radius_worst            | "worst" or largest mean value for mean of distances from center to points on the perimeter                                                                       | numerical                      | Range between 7.93-36.04         |
| 24  | texture_worst           | "worst" or largest mean value for standard deviation of gray-scale values                                                                                        | numerical                      | Range between 12.02-49.54        |
| 25  | perimeter_worst         | "worst" or largest mean value for the size of the core tumor                                                                                                     | numerical                      | Range between 50.41-251.20       |
| 26  | area_worst              | "worst" or largest mean value for total area occupied by the nucleus.                                                                                            | numerical                      | Range between 185.2-4254.0       |
| 27  | smoothness_worst        | "worst" or largest mean value for local variation in radius lengths                                                                                              | numerical                      | Range between 0.07-0.22          |
| 28  | compactness_worst       | "worst" or largest mean value for perimeter\^2 / area - 1.0                                                                                                      | numerical                      | Range between 0.02-1.05          |
| 29  | concavity_worst         | "worst" or largest mean value for severity of concave portions of the contour                                                                                    | numerical                      | Range between 0.00-1.2           |
| 30  | concave.points_worst    | "worst" or largest mean value for number of concave portions of the contour                                                                                      | numerical                      | Range between 0.00-0.29          |
| 31  | symmetry_worst          | "worst" or largest mean value that quantifies the degree to which the shape of the nuclei is symmetrical.                                                        | numerical                      | Range between 0.1-0.6            |
| 32  | fractal_dimension_worst | "worst" or largest mean value for "coastline approximation" - 1                                                                                                  | numerical                      | Range between 0.05-0.20          |
| 33  | X                       | has no value all records are missing "NA"                                                                                                                        | logical                        | "NA"                             |

# 4. Code

```{r}
library("tidyverse")
library("dplyr")
library("readr")
library("Hmisc")
library("outliers")
library("mlbench")
library("ggplot2")
library("lattice")
library("caret")
library("caTools")
library("corrplot")
library("reshape2")
```

```{r}
data <- read_csv("C:/Users/warif/OneDrive/Desktop/mining project data/data.csv")
head(data)
```

```{r}
nrow(data)
```

```{r}
ncol(data)
```

```{r}
dim(data)
```

```{r}
names(data)
```

```{r}
str(data)
```

```{r}
summary(data)
```

```{r}
describe(data)
```

```{r}
data1 <- data
head(data1)
```

### Data reduction

We had an attribute in our dataset that was entirely made up of null values, thus we had to eliminate it under dimensionality reduction, attribute subset reduction, because it was an irrelevant attribute that contained no useful information.

All the other attributes were relevant to the class label where they describe the mass such as its radius, area or the texture, but the id attribute is not relevant to the class label since it does not describe the mass in any sense, so we had to perform dimensionality reduction that falls under attribute subset reduction, since the id didn't contain any useful information.

```{r}
data1 <- subset(data1, select = -...33)
data1 <- subset(data1, select = -id)
data1 <- data1[, c(2:ncol(data1), 1)]
head(data1)
```

## Visualization

One of the main goals of visualizing the data here is to observe which features are most helpful in predicting malignant or benign cancer. The other is to see general trends that may aid us in model selection and hyper parameter selection.

### 1-pie chart

M= Malignant (cancerous); B= Benign (non-cancerous) 357 observations which account for 62.7% of all observations indicating the absence of cancer cells, 212 which account for 37.3% of all observations shows the presence of cancerous cell.

```{r}
diagnosis.table <- table(data$diagnosis)
colors <- terrain.colors(2) 
# a pie chart for the diagnosis 
diagnosis.prop.table <- prop.table(diagnosis.table)*100
diagnosis.prop.df <- as.data.frame(diagnosis.prop.table)
pielabels <- sprintf("%s - %3.1f%s", diagnosis.prop.df[,1], diagnosis.prop.table, "%")
colors = c ("pink", "skyblue1")
pie(diagnosis.prop.table,
    labels=pielabels,  
    clockwise=TRUE,
    col=colors,
    border="gainsboro",
    radius=0.8,
    cex=0.8, 
    main="Frequency Of Cancer Diagnosis")
legend(1, .4, legend=diagnosis.prop.df[,1], cex = 0.7,fill = colors)
```

```{r}
#Remove the first  column
data2 <- data[, -1]
#Remove the last column
data2 <- data2[,-32]
#Tidy the data
#bc_data$diagnosis <- as.factor(bc_data$diagnosis)
summary(data2)
```

```{r}
head(data2)
```

### 2-correlations

correlations used to analyze how the 30 attributes relate to each other, using the correlation function cor() we notice which. Attributes are highly correlated and produce redundant data such as the columns "compactness_mean", "concavity_mean", "texture_worst", "fractal_dimension_se", "texture_mean", "perimeter_worst", "diagnosis", "texture_se", "perimeter_se" and "radius_mean".

```{r}
#correlations
c <- cor(data2[,2:31])
corrplot(c, order = "hclust", tl.cex = 0.7)
```

## 3-covariance

```{r}
#covariance 
cov(data2[, 2:31])
```

### 4-Boxplot

Comparing the raidus, area and concavity of benging malignant stage By constructing a box plot with the function ggplot()+geom_boxplot(), we notice malignant cells have higher radius, area and concavity mean than benign cell which is an information we can use later for predicting and training the data.

```{r}
# Comparing the raidus, area and concavity of benging malignant stage
ggplot(data2, aes(x=diagnosis, y=radius_mean))+geom_boxplot()+ggtitle("area of bengin Vs malignant")
```

```{r}
ggplot(data2, aes(x=diagnosis, y=concavity_mean))+geom_boxplot()+ggtitle("concavity of bengin Vs malignant")
```

```{r}
# Reshape the data
data_long <- data2 %>%
  pivot_longer(cols = -diagnosis, names_to = "variable", values_to = "value")

# Plot box plots with separate y-axis scales and arranged in a grid
ggplot(data_long, aes(x = diagnosis, y = value)) +
  geom_boxplot() +
  facet_wrap(~ variable, nrow = 5, ncol = 6, scales = "free_y") +
  ggtitle("Box Plot for All Columns with Diagnosis") 
```

### 5-Visualise distribution of data(histograms Plot)

#### a-Histograms of data "\_mean":

1.  we can see that radius_mean of malignant tumors are bigger than radius_mean of benign tumors mostly.
2.  The benign distribution (blue in graph) is approcimately bell-shaped that is shape of normal distribution. However the same is not true for the malignant class data.
3.  Also the mean value of malignant is higher than that of benign class.

```{r}
#Break up columns into groups, according to their suffix designation 
#(_mean, _se,and __worst) to perform visualisation plots off.
data_mean <- data[ ,c("diagnosis", "radius_mean", "texture_mean","perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave points_mean", "symmetry_mean", "fractal_dimension_mean" )]

data_se <- data[ ,c("diagnosis", "radius_se", "texture_se","perimeter_se", "area_se", "smoothness_se", "compactness_se", "concavity_se", "concave points_se", "symmetry_se", "fractal_dimension_se" )]

data_worst <- data[ ,c("diagnosis", "radius_worst", "texture_worst","perimeter_worst", "area_worst", "smoothness_worst", "compactness_worst", "concavity_worst", "concave points_worst", "symmetry_worst", "fractal_dimension_worst" )]
```

```{r}
#Plot histograms of "_mean" variables group by diagnosis
ggplot(data = melt(data_mean, id.var = "diagnosis"), mapping = aes(x = value)) + 
  geom_histogram(bins = 10, aes(fill=diagnosis), alpha=0.5) + facet_wrap(~variable, scales =      'free_x')
```

#### b- Histograms of data "\_se","\_worst":

Most of the features are normally distributed. Comparison of radius distribution by malignancy shows that there is no perfect separation between any of the features; we do have fairly good separations for concave.points_worst, concavity_worst, perimeter_worst, area_mean, perimeter_mean.

```{r}
#Plot histograms of "_se" variables group by diagnosis
ggplot(data = melt(data_se, id.var = "diagnosis"), mapping = aes(x = value)) + 
  geom_histogram(bins = 10, aes(fill=diagnosis), alpha=0.5) + facet_wrap(~variable, scales = 'free_x')
```

```{r}
#Plot histograms of "_worst" variables group by diagnosis
ggplot(data = melt(data_worst, id.var = "diagnosis"), mapping = aes(x = value)) + 
  geom_histogram(bins = 10, aes(fill=diagnosis), alpha=0.5) + facet_wrap(~variable, scales = 'free_x')
```

# 5.Data preprocessing

Data preprocessing is a vital step in data mining, it involves transforming raw data into a format that is suitable for modeling. The process of data preprocessing contains various techniques and methods to clean, transform, and prepare data before it can be utilized for training models or extracting meaningful insights.

### 1- Data cleaning

Data cleaning involves identifying and correcting or removing inconsistencies, errors, and inaccuracies in the dataset. Missing values need to be handled appropriately to avoid bias or distortion in the analysis. Outliers, which are extreme values that deviate significantly from the majority of the data, can impact the accuracy of models and need to be detected and addressed.

Fortunately, we don't have any missing values in our dataset, but we were able to identify a total of 265 outliers using the z-score method. The number of outliers for each attribute ranged between 65 and 0, with area_se having the most outliers, and texture_worst, smoothness_worst, and concave points_worst having the fewest outliers.

We did not remove all the outlier objects because there were over 100 of them and removing all of them could compromise the data's integrity. To settle the issue, we decided to remove the most frequent outlier objects.

To know which objects were the most frequent outliers, we counted the number of times an object became an outlier to a certain attribute, it ranged between 11 to 0, with object number 462 being the most frequent outlier, then we decided that every object that was an outlier more than five times will be deleted, therefore we deleted a total of 13 object.

```{r}
#Checking NULL, FALSE means no null, TRUE cells means the value of the cell is null
is.na(data1)
```

```{r}
# to find the total null values in the dataset
sum(is.na(data1))
```

#### Outliers

```{r}
data2 <- subset(data1, select = -diagnosis)
# Function to identify outliers using z-score
find_outliers <- function(column) {
  median_val <- median(column)
  mad_val <- mad(column)
  modified_z_scores <- 0.6745 * (column - median_val) / mad_val
  return(which(abs(modified_z_scores) > 3))
}
# Function to count outliers for each attribute
count_outliers <- function(data) {
  outlier_counts <- sapply(data, function(col) length(find_outliers(col)))
  return(outlier_counts)
}
# Apply the count_outliers function to the data frame
outlier_counts <- count_outliers(data2)
# Print the outlier counts for each attribute
cat("Outlier counts for each attribute:\n")
```

```{r}
print(outlier_counts)
```

```{r}
# Create an empty list to store outlier indices for each attribute
outlier_indices <- list()
# Apply the find_outliers function to each attribute in the data frame
for (col in names(data2)) {
  outlier_indices[[col]] <- find_outliers(data2[[col]])
}
# Print the outlier indices
print(outlier_indices)
```

```{r}
# Flatten the outlier indices into a single vector
flattened_indices <- unlist(outlier_indices)
# Get the outlier counts
outlier_counts <- table(flattened_indices)
# Print the outlier counts
cat("Outlier counts:\n")
```

```{r}
print(outlier_counts)
```

```{r}
# Get the total count of outliers
total_outliers <- sum(outlier_counts)
# Print the total count of outliers
cat("Total count of outliers:", total_outliers, "\n")
```

```{r}
# Get the most frequent outlier objects 
most_frequent_outlier_objects <- if (length(flattened_indices) > 0) {
  table(flattened_indices)
} else {
  NULL
}
# Sort the most frequent outlier objects in descending order
sorted_most_frequent_outliers <- sort(most_frequent_outlier_objects, decreasing = TRUE)
# Get the outlier objects with a frequency greater than 5
frequent_outliers <- sorted_most_frequent_outliers[sorted_most_frequent_outliers > 5]
# Print the outlier objects with their frequencies
cat("Outlier objects with a frequency greater than 5:\n")
```

```{r}
for (row in names(frequent_outliers)) {
  cat("Object", row, "- Frequency:", frequent_outliers[row], "\n")
}
```

```{r}
# Identify the objects to be deleted
objects_to_delete <- as.integer(names(frequent_outliers))
# Delete the objects from the data frame
data1 <- data1[-objects_to_delete, ]
```

### 2- Data reduction

We had an attribute in our dataset that was entirely made up of null values, thus we had to eliminate it under dimensionality reduction, attribute subset reduction, because it was an irrelevant attribute that contained no useful information.

All the other attributes were relevant to the class label where they describe the mass such as its radius, area or the texture, but the id attribute is not relevant to the class label since it does not describe the mass in any sense, so we had to perform dimensionality reduction that falls under attribute subset reduction, since the id didn't contain any useful information.

### 3- Attribute transformation

After data reduction, all the remaining attributes were numeric except for the diagnosis, which was nominal, precisely categorical Asymmetric Binary, where M stands for malignant and B stands for benign, so we had to perform an attribute transformation on the diagnosis in which we encoded every B to become 0, and every M to become 1.

```{r}
data1$diagnosis <- ifelse(data1$diagnosis == "M", 1, ifelse(data1$diagnosis == "B", 0, data1$diagnosis))
# Convert the target variable to numeric or logical
data1$diagnosis <- as.numeric(data1$diagnosis)
head(data1)
```

### 4- Feature scaling

The scale() function is utilized to calculate the mean and standard deviation of the selected columns (1 to 30). Each element in the columns is then scaled by subtracting the mean and dividing by the standard deviation. Feature scaling is essential as it brings the features to a standardized range, preventing any single feature from dominating the learning process due to differences in scale.

```{r}
#scale, with default settings, will calculate the mean and standard deviation of the entire vector, then "scale" each element by those values by subtracting the mean and dividing by the sd
data1 [, 1:30] = scale(data1 [, 1:30])
head(data1)
```

### 5- Normalization

Normalization is a preprocessing technique that scales numerical features to a consistent range, typically between 0 and 1. By applying the normalize() function to the selected columns, the code ensures that no single feature dominates the learning process due to differences in scale. This normalization step is valuable for improving the performance and stability of data mining algorithms.

```{r}
# Define function normalize()
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}
cols_to_normalize <- 1:(ncol(data1) - 1)  # Select all columns except the last one
data1[, cols_to_normalize] <- lapply(data1[, cols_to_normalize], normalize)
head(data1)
```

### 6- Discretization

Based on the selected data set we decided to use equal width in discretization process , performed on the selected columns. We selected all numeric countinues attributes (In this case, all columns except the last one were selected), and divided using equal width with 20 bins.

The discretization process involved applying the equal_width_discretize() function to each column selected using lapply(). And to handle null data we Replaced null values with value = 0. The resulting discrete values were then mapped back to the data set, data1, and displayed using View().

The reason for choosing an equal-width distribution in this case is to divide the range of values in each selected column into a fixed number of equal-sized bins. This provides us with several features that help us in analysis:

1- Simplicity and interpretability: Equal width divides the range of values into equal-sized intervals, making the resulting categories easier to interpret.

2- Preserving the distribution: By dividing the range into equal intervals, it ensures that each bin covers an equal width of the data range, allowing for a more balanced representation of values.

```{r}
# Number of bins for discretization
num_bins <- 20
# Function to perform discretization using equal width
equal_width_discretize <- function(var, num_bins) {
  breaks <- seq(min(var, na.rm = TRUE), max(var, na.rm = TRUE), length.out = num_bins + 1)
  cut(var, breaks = breaks, labels = FALSE)
}
# Subset the dataset with selected columns
cols_to_Discretiz <- 1:(ncol(data1) - 1)  # Select all columns except the last one
# Apply equal width discretization to selected columns
data1_discretized <- lapply(data1[, cols_to_Discretiz], equal_width_discretize, num_bins)
# Replace null values with 0
data1_discretized <- lapply(data1_discretized, function(x) ifelse(is.na(x), 0, x))
# Assign discretized values back to the original dataset
data1[, cols_to_Discretiz] <- data1_discretized
# View the modified dataset
head(data1)
```

### 7- Feature selection

We performed a feature selection by assessing the importance of attributes in the dataset. The code trains a linear regression model and calculates the importance of each attribute. The least important 9 attributes were identified by their importance being under 0.5, these attributes were symmetry_se, fractal_dimension_mean, texture_mean, concave points_se, perimeter_se, symmetry_mean, compactness_se, smoothness_mean, and compactness_worst, while the highest importance being 3.05 and belonging to compactness_mean. The least important 9 attributes are subsequently recommended for removal, removing the least important 9 attributes can be beneficial for several reasons:

Dimensionality reduction: Removing less important attributes reduces the dimensionality of the dataset, making it more manageable. It simplifies the model and can lead to improved model performance.

Computational efficiency: With fewer attributes, the training and evaluation of the model become faster and more efficient. Removing unnecessary attributes reduces the computational cost associated with processing and analyzing the data.

Simplifying the model: By focusing on the most important attributes, the model becomes more interpretable and easier to understand. It allows for better insights into the relationships between the selected attributes and the target variable.

```{r}
# ensure results are repeatable
set.seed(7)
# prepare training scheme
control4 <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
# train the model
model4 <- train(diagnosis ~.,data = data1 , method="lm", preProcess="scale", trControl=control4)
# estimate variable importance
importance4 <- varImp(model4 , scale=FALSE)
# plot importance
plot(importance4)
```

```{r}
# train the model
model4 <- lm(diagnosis ~.,data = data1)
# estimate variable importance
importance4 <- varImp(model4 , scale=FALSE)
# Copy the importance3 dataframe to a new dataframe
importance4_copy <- importance4
# Add a column for row names
importance4_copy$name <- row.names(importance4_copy)
# Sort the dataframe based on the "Overall" column in descending order
importance4_copy <- importance4_copy[order(importance4_copy$Overall, decreasing = TRUE), ]
# Reset row names
row.names(importance4_copy) <- NULL
# Print the rearranged dataframe
print(importance4_copy)
```

```{r}
# Remove the least important 9 attributes
data1 <- subset(data1, select = -symmetry_se)
data1 <- subset(data1, select = -fractal_dimension_mean)
data1 <- subset(data1, select = -texture_mean)
data1 <- subset(data1, select = -`concave points_se`)
data1 <- subset(data1, select = -perimeter_se)
data1 <- subset(data1, select = -symmetry_mean)
data1 <- subset(data1, select = -compactness_se)
data1 <- subset(data1, select = -smoothness_mean)
data1 <- subset(data1, select = -compactness_worst)
head(data1)
```

## Data before preprocessing

```{r}
#Data before preprocessing
rawData <- data
head(rawData)
```

## Preprocessed data

```{r}
#Preprocessed data
preprocessedData <- data1
head(preprocessedData)
```

# 6. Classification

### 1- Convert the response variable to a factor

The objective of this script is to prepare the dataset for classification analysis by converting the response variable into a factor. The "diagnosis," is transformed into a factor with levels 0 and 1.

To enhance the clarity and consistency of the preprocessedData dataset by renaming specific columns. Specifically, columns with indices 6 and 19 are renamed to "concave_points_mean" and "concave_points_worst," respectively, to address the presence of spaces in their original names.

```{r}
colnames(preprocessedData)[6] <- "concave_points_mean"
colnames(preprocessedData)[19] <- "concave_points_worst"
preprocessedData$diagnosis <- factor(preprocessedData$diagnosis, levels = c(0, 1))
```

```{r}
# Load required libraries
library(party)
library(FSelector)
library(partykit)
library(caret)
```

### 2- Partitioning Method

We used the Holdout Partitioning Method to partition the preprocessed dataset into training and testing sets using different partition methods. The three partition methods involve creating splits of 70-30, 60-40, and 80-20 ratios between the training and testing sets.

The holdout method is a widely used technique in Data mining for partitioning a dataset into training and testing sets. It involves splitting the dataset into two subsets: one for training the model and the other for evaluating its performance. The primary reasons for the popularity of the holdout method include its simplicity, ease of implementation, and ability to provide a quick assessment of a model's performance.

#### a- Seed for Reproducibility:

The code begins by setting a seed (seed value: 7) to ensure reproducibility. This is a good practice, especially when random processes are involved in the data partitioning.

```{r}
# Set seed for reproducibility
set.seed(7)

# Different partition methods (70-30, 60-40, and 80-20 splits)
ind1 <- sample(2, nrow(preprocessedData), replace=TRUE, prob=c(0.7, 0.3))
ind2 <- sample(2, nrow(preprocessedData), replace=TRUE, prob=c(0.6, 0.4))
ind3 <- sample(2, nrow(preprocessedData), replace=TRUE, prob=c(0.8, 0.2))

trainData1 <- preprocessedData[ind1 == 1,]
testData1 <- preprocessedData[ind1 == 2,]

trainData2 <- preprocessedData[ind2 == 1,]
testData2 <- preprocessedData[ind2 == 2,]

trainData3 <- preprocessedData[ind3 == 1,]
testData3 <- preprocessedData[ind3 == 2,]
```

Prepare the training and testing datasets (trainData1, trainData2, trainData3, testData1, testData2, testData3) for machine learning modeling by converting the "diagnosis" variable into a factor variable. Additionally, it ensures consistent factor levels between the training and testing datasets.

```{r}
# Convert diagnosis to a factor variable
trainData1$diagnosis <- factor(trainData1$diagnosis, levels = c(0, 1))
trainData2$diagnosis <- factor(trainData2$diagnosis, levels = c(0, 1))
trainData3$diagnosis <- factor(trainData3$diagnosis, levels = c(0, 1))

# Ensure consistent factor levels in training and test data
testData1$diagnosis <- factor(testData1$diagnosis, levels = c(0, 1))
testData2$diagnosis <- factor(testData2$diagnosis, levels = c(0, 1))
testData3$diagnosis <- factor(testData3$diagnosis, levels = c(0, 1))
```

### 3- Attribute selecting measures

#### a- Information Gain

To develop decision tree models using the Information Gain method for attribute selection. The C50 algorithm is employed for this purpose. The script creates three decision tree models with different training datasets, each representing a distinct split ratio (70-30, 60-40, and 80-20).

##### 1. library of Information Gain trees

The C5.0 function from the C50 package in R does not have a direct argument to specify the attribute selection measure (such as Information Gain, Information Gain Ratio, or Gini Index), However, we used the C5.0() function without specifying the attribute selection measure, and it will automatically use Information Gain for decision tree construction because Information Gain is the default attribute selecting measure.

```{r}

#Information Gain method for selecting attributes using C50 algorithm
# Load required library for Information Gain
library(C50)

# Set seed for reproducibility
set.seed(7)

# Define the formula for the decision tree
formula_info_gain <- diagnosis ~ .

# Fit decision tree models using Information Gain
# Build decision tree model using Information Gain selected attributes (70-30 split)
model_info_gain1 <- C5.0(formula_info_gain, data = trainData1)
# Build decision tree model using Information Gain selected attributes (60-40 split)
model_info_gain2 <- C5.0(formula_info_gain, data = trainData2)
# Build decision tree model using Information Gain selected attributes (80-20 split)
model_info_gain3 <- C5.0(formula_info_gain, data = trainData3)


```

##### 2. plot Information Gain trees

Visually represent the decision trees generated using the Information Gain method with different partition methods (70-30, 60-40, and 80-20 splits). The C50 algorithm is employed for building these decision tree models.

```{r}
# Plot the decision trees for different partition methods (Information Gain)
# Plot decision tree for (70-30 split)
plot(model_info_gain1, main = "Information Gain Decision Tree (70-30 split)")
```

```{r}
# Plot decision tree for (60-40 split)
plot(model_info_gain2, main = "Information Gain Decision Tree (60-40 split)")
```

```{r}
# Plot decision tree for (80-20 split)
plot(model_info_gain3, main = "Information Gain Decision Tree (80-20 split)")
```

##### 3. Make predictions of Information Gain trees

Make predictions using the decision tree models generated with the Information Gain method. Three sets of predictions are made based on different partition methods (70-30, 60-40, and 80-20 splits). The predictive power of decision tree models is assessed by applying them to previously unseen data. In this case, three decision tree models, generated with varying training and testing splits, are used to predict the class labels of the corresponding test datasets.

```{r}
#(70-30 split)
predictions_info_gain1 <- predict(model_info_gain1, testData1)
#(60-40 split)
predictions_info_gain2 <- predict(model_info_gain2, testData2)
#(80-20 split)
predictions_info_gain3 <- predict(model_info_gain3, testData3)
```

#### 4. Evaluate the models of Information Gain trees

#### 1. Information Gain trees

we present a comprehensive evaluation of decision tree models generated using the Information Gain method. The assessment includes key performance metrics such as accuracy, precision, sensitivity, and specificity for each model. Additionally, confusion matrices are employed to offer a detailed breakdown of model predictions.

#### 2. Methodology

2.1. Accuracy Calculation The accuracy of each model is computed by comparing the predicted values against the actual values in the test set.

2.2. Precision, Sensitivity, Specificity Calculation A custom function, calculate_metrics, is utilized to compute precision, sensitivity, and specificity based on the confusion matrix.

2.3. Confusion Matrix Generation The conf_matrix function constructs a confusion matrix using the confusionMatrix function from the caret package.

2.4. Print Metrics for Each Split Metrics such as accuracy, precision, sensitivity, specificity, and the confusion matrix are printed for each split to provide a detailed evaluation.

#### 3. Results

3.1. Accuracy (70-30 Split) The accuracy of the Information Gain decision tree model is approximately 89.66%. This implies that the model correctly predicts class labels in nearly 90% of cases on the test set.

(60-40 Split) The accuracy increases to around 93.93% for the 60-40 split, indicating improved performance compared to the 70-30 split.

(80-20 Split) For the 80-20 split, the accuracy is approximately 92.24%, demonstrating consistent performance across different data splits.

3.2. Precision Precision measures the proportion of true positive predictions among all positive predictions.

(70-30 Split) The precision is approximately 88.57%, indicating that when the model predicts the positive class, it is correct around 88.57% of the time.

(60-40 Split) Precision increases to about 92.11% for the 60-40 split, reflecting a higher proportion of true positive predictions among positive predictions.

(80-20 Split) The precision further improves to around 95.12% for the 80-20 split, suggesting high correctness in predicting positive instances.

3.3. Sensitivity Sensitivity measures the proportion of true positive predictions among all actual positive cases.

(70-30 Split) Sensitivity is approximately 86.11%, indicating that the model captures around 86.11% of actual positive instances.

(60-40 Split) Sensitivity increases to about 90.91%, indicating improved performance in correctly identifying positive instances for the 60-40 split.

(80-20 Split) Sensitivity decreases slightly to around 84.78%, suggesting that the model captures a slightly lower proportion of actual positive instances for the 80-20 split.

3.4. Specificity Specificity reflects the proportion of true negative predictions among all actual negative cases.

(70-30 Split) Specificity is approximately 92.16%, indicating a high proportion of correct predictions for the negative class.

(60-40 Split) Specificity increases to about 95.62%, demonstrating improved correctness in predicting negative instances for the 60-40 split.

(80-20 Split) Specificity reaches around 97.14%, showing high correctness in predicting negative instances for the 80-20 split.

3.5. Confusion Matrices Confusion matrices provide a detailed breakdown of model predictions.

(70-30 Split) The confusion matrix shows that out of 102 actual positive instances, the model correctly predicted 88 (true positives), and out of 102 actual negative instances, the model correctly predicted 94 (true negatives).

(60-40 Split) The confusion matrix indicates that out of 77 actual positive instances, the model correctly predicted 70 (true positives), and out of 137 actual negative instances, the model correctly predicted 131 (true negatives).

(80-20 Split) The confusion matrix reveals that out of 41 actual positive instances, the model correctly predicted 39 (true positives), and out of 70 actual negative instances, the model correctly predicted 68 (true negatives).

#### 4. Overall Assessment

The Information Gain decision tree models exhibit robust performance across various data splits, showcasing high accuracy and maintaining a balanced interplay between precision, sensitivity, and specificity. Among the evaluated splits, the (60-40 Split) configuration emerges as the optimal choice, considering the comprehensive evaluation metrics.

Reasons for (60-40 Split) Preference:

1- Optimal Accuracy: The Information Gain decision tree model achieves the highest accuracy of approximately 93.93% for the 60-40 split, surpassing both the 70-30 and 80-20 splits. This implies superior predictive capabilities on the test set.

2- Balanced Precision and Sensitivity: Precision increases to about 92.11% for the 60-40 split, indicating a higher proportion of true positive predictions among positive predictions. Sensitivity remains strong at around 90.91%, demonstrating the model's effectiveness in correctly identifying positive instances.

3- High Specificity: Specificity increases to about 95.62% for the 60-40 split, showcasing improved correctness in predicting negative instances. This is crucial for minimizing false negatives and ensuring reliability in identifying non-positive cases.

4- Consistent Performance: The (60-40 Split) maintains a balance between accuracy and other key metrics, demonstrating consistent and reliable performance across different aspects of model evaluation.

While the model's performance remains commendable across all splits, the (60-40 Split) is recommended as it represents a favorable compromise, providing high accuracy and a well-rounded performance profile.

```{r}
# Evaluate the models accuracy
#(70-30 split)
accuracy_info_gain1 <- sum(predictions_info_gain1 == testData1$diagnosis) / nrow(testData1)
#(60-40 split)
accuracy_info_gain2 <- sum(predictions_info_gain2 == testData2$diagnosis) / nrow(testData2)
#(80-20 split)
accuracy_info_gain3 <- sum(predictions_info_gain3 == testData3$diagnosis) / nrow(testData3)

# Function to calculate Precision, Sensitivity, Specificity
calculate_metrics <- function(conf_matrix) {
  precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
  sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
  specificity <- conf_matrix[1, 1] / sum(conf_matrix[1, ])
  
  return(c(precision, sensitivity, specificity))
}

# Confusion matrix function
conf_matrix <- function(predictions, actual) {
  result <- confusionMatrix(table(predictions, actual))
  result$table
}

#(70-30 split)
conf_matrix_info_gain1 <- conf_matrix(predictions_info_gain1, testData1$diagnosis)
metrics_info_gain1 <- calculate_metrics(conf_matrix_info_gain1)

#(60-40 split)
conf_matrix_info_gain2 <- conf_matrix(predictions_info_gain2, testData2$diagnosis)
metrics_info_gain2 <- calculate_metrics(conf_matrix_info_gain2)

#(80-20 split)
conf_matrix_info_gain3 <- conf_matrix(predictions_info_gain3, testData3$diagnosis)
metrics_info_gain3 <- calculate_metrics(conf_matrix_info_gain3)
```

```{r}
# Print accuracies and additional metrics for comparison
print("Evaluation Metrics for Information Gain with (70-30 split):")
print(paste("Accuracy:", accuracy_info_gain1))
print(paste("Precision:", metrics_info_gain1[1]))
print(paste("Sensitivity:", metrics_info_gain1[2]))
print(paste("Specificity:", metrics_info_gain1[3]))
print("Confusion Matrix:")
print(conf_matrix_info_gain1)

print("Evaluation Metrics for Information Gain with (60-40 split):")
print(paste("Accuracy:", accuracy_info_gain2))
print(paste("Precision:", metrics_info_gain2[1]))
print(paste("Sensitivity:", metrics_info_gain2[2]))
print(paste("Specificity:", metrics_info_gain2[3]))
print("Confusion Matrix:")
print(conf_matrix_info_gain2)

print("Evaluation Metrics for Information Gain with (80-20 split):")
print(paste("Accuracy:", accuracy_info_gain3))
print(paste("Precision:", metrics_info_gain3[1]))
print(paste("Sensitivity:", metrics_info_gain3[2]))
print(paste("Specificity:", metrics_info_gain3[3]))
print("Confusion Matrix:")
print(conf_matrix_info_gain3)
```

#### b- Gini Index

This section focuses on constructing decision tree models using the Gini Index method for attribute selection. The decision tree models are built with the rpart algorithm, and the resulting trees are visualized using the rpart.plot library. The goal is to evaluate the performance of the models on different data splits.

#### 1. library of Gini Index trees

The necessary libraries, rpart and rpart.plot, are loaded to facilitate decision tree construction and visualization. rpart package in R has a direct argument to specify the attribute selection measure such as Gini Index, so used The parms argument with list(split = "gini") in the rpart function to specify additional parameters for the tree-building process. In this case, it indicates the use of the Gini Index as the criterion for splitting nodes in the decision tree.

```{r}
#Gini Index method for selecting attributes using rpart algorithm
# Load required library for gini
library(rpart)
library(rpart.plot)

# Set seed for reproducibility
set.seed(7)

# Define the formula for the decision tree
formula_gini <- diagnosis ~ .

# Fit decision tree models using Gini Index
# Build decision tree model using Gini Index selected attributes (70-30 split)
model_gini1 <- rpart(formula_gini, data = trainData1, method = "class", parms = list(split = "gini"))
# Build decision tree model using Gini Index selected attributes (60-40 split)
model_gini2 <- rpart(formula_gini, data = trainData2, method = "class", parms = list(split = "gini"))
# Build decision tree model using Gini Index selected attributes (80-20 split)
model_gini3 <- rpart(formula_gini, data = trainData3, method = "class", parms = list(split = "gini"))
```

#### 2. plot Gini Index trees

Visualizing decision trees constructed using the Gini Index method for attribute selection. The decision trees are generated based on different partition methods (70-30, 60-40, and 80-20 splits) to observe how the tree structures vary with different training and testing data subsets.

```{r}
# Plot the decision trees for different partition methods (Gini Index)
# Plot decision tree for (70-30 split)
rpart.plot(model_gini1, type = 2, box.palette = c("lightblue", "lightgreen"), fallen.leaves = TRUE, main = "Gini Index Decision Tree (70-30 split)")
```

```{r}
# Plot decision tree for (60-40 split)
rpart.plot(model_gini2, type = 2, box.palette = c("lightblue", "lightgreen"), fallen.leaves = TRUE, main = "Gini Index Decision Tree (60-40 split)")
```

```{r}
# Plot decision tree for (80-20 split)
rpart.plot(model_gini3, type = 2, box.palette = c("lightblue", "lightgreen"), fallen.leaves = TRUE, main = "Gini Index Decision Tree (80-20 split)")
```

#### 3. Make predictions of Gini Index trees

Making predictions using decision tree models generated through the Gini Index method. After constructing decision trees based on different partition methods (70-30, 60-40, and 80-20 splits), this code employs these models to predict the class labels of the test datasets.

```{r}
# Make predictions
#(70-30 split)
predictions_gini1 <- predict(model_gini1, testData1, type = "class")
#(60-40 split)
predictions_gini2 <- predict(model_gini2, testData2, type = "class")
#(80-20 split)
predictions_gini3 <- predict(model_gini3, testData3, type = "class")
```

#### 4. Evaluate the models of Gini Index trees

#### 1. Gini Index Decision Trees

We present a thorough evaluation of decision tree models generated using the Gini Index method. The assessment encompasses key performance metrics such as accuracy, precision, sensitivity, and specificity for each model. Additionally, confusion matrices are utilized to provide a detailed breakdown of model predictions.

#### 2. Methodology

2.1. Accuracy Calculation The accuracy of each model is computed by comparing the predicted values against the actual values in the test set.

2.2. Precision, Sensitivity, Specificity Calculation A custom function, calculate_metrics, is employed to compute precision, sensitivity, and specificity based on the confusion matrix.

2.3. Confusion Matrix Generation The conf_matrix function constructs a confusion matrix using the confusionMatrix function from the caret package.

2.4. Print Metrics for Each Split Metrics such as accuracy, precision, sensitivity, specificity, and the confusion matrix are printed for each split to provide a detailed evaluation.

#### 3. Results

3.1. Accuracy (70-30 Split) The accuracy of the Gini Index decision tree model is approximately 88.51%. This implies that the model correctly predicts class labels in nearly 90% of cases on the test set.

(60-40 Split) The accuracy increases to around 90.19% for the 60-40 split, indicating improved performance compared to the 70-30 split.

(80-20 Split) For the 80-20 split, the accuracy is approximately 93.97%, demonstrating consistent performance across different data splits.

3.2. Precision Precision measures the proportion of true positive predictions among all positive predictions.

(70-30 Split) The precision is approximately 77.14%, indicating that when the model predicts the positive class, it is correct around 77.14% of the time.

(60-40 Split) Precision increases to about 92.11%, reflecting a higher proportion of true positive predictions among positive predictions.

(80-20 Split) The precision further improves to around 95.12%, suggesting high correctness in predicting positive instances.

3.3. Sensitivity Sensitivity measures the proportion of true positive predictions among all actual positive cases.

(70-30 Split) Sensitivity is approximately 93.10%, indicating that the model captures around 93.10% of actual positive instances.

(60-40 Split) Sensitivity increases to about 82.35%, indicating improved performance in correctly identifying positive instances for the 60-40 split.

(80-20 Split) Sensitivity decreases slightly to around 88.64%, suggesting that the model captures a slightly lower proportion of actual positive instances for the 80-20 split.

3.4. Specificity Specificity reflects the proportion of true negative predictions among all actual negative cases.

(70-30 Split) Specificity is approximately 86.21%, indicating a high proportion of correct predictions for the negative class.

(60-40 Split) Specificity increases to about 95.35%, demonstrating improved correctness in predicting negative instances for the 60-40 split.

(80-20 Split) Specificity reaches around 97.22%, showing high correctness in predicting negative instances for the 80-20 split.

3.5. Confusion Matrices Confusion matrices provide a detailed breakdown of model predictions.

(70-30 Split) The confusion matrix shows that out of 102 actual positive instances, the model correctly predicted 88 (true positives), and out of 102 actual negative instances, the model correctly predicted 94 (true negatives).

(60-40 Split) The confusion matrix indicates that out of 77 actual positive instances, the model correctly predicted 70 (true positives), and out of 137 actual negative instances, the model correctly predicted 131 (true negatives).

(80-20 Split) The confusion matrix reveals that out of 41 actual positive instances, the model correctly predicted 39 (true positives), and out of 70 actual negative instances, the model correctly predicted 68 (true negatives).

#### 4. Overall Assessment

The Gini Index decision tree models demonstrate consistent and commendable performance across different data splits, showcasing high accuracy and a well-balanced combination of precision, sensitivity, and specificity. The assessment indicates that the Gini Index decision tree model achieves the best performance with the (80-20 Split) configuration.

Reasons for (80-20 Split) Outperformance:

1- Higher Accuracy: The model achieves the highest accuracy of approximately 93.97% for the 80-20 split, indicating superior predictive capabilities compared to other splits.

2- Balanced Precision and Sensitivity: The precision for positive predictions is notably high at around 95.12%, suggesting a high correctness in predicting positive instances. Although sensitivity slightly decreases to around 88.64%, it remains at a respectable level.

3-Outstanding Specificity: Specificity reaches approximately 97.22%, indicating exceptional correctness in predicting negative instances. This is crucial for minimizing false negatives and ensuring the model's reliability in identifying non-positive cases.

While the model performs well across different splits, the (80-20 Split) configuration stands out as the most favorable based on the overall balance of accuracy, precision, sensitivity, and specificity. Further analysis and potential model tuning can be explored to enhance predictive capabilities and address specific use-case requirements.

```{r}
# Evaluate the models accuracy
#(70-30 split)
accuracy_gini1 <- sum(predictions_gini1 == testData1$diagnosis) / nrow(testData1)
#(60-40 split)
accuracy_gini2 <- sum(predictions_gini2 == testData2$diagnosis) / nrow(testData2)
#(80-20 split)
accuracy_gini3 <- sum(predictions_gini3 == testData3$diagnosis) / nrow(testData3)

# Function to calculate Precision, Sensitivity, Specificity
calculate_metrics <- function(conf_matrix) {
  precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
  sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
  specificity <- conf_matrix[1, 1] / sum(conf_matrix[1, ])
  c(precision, sensitivity, specificity)
}

# Confusion matrix function
conf_matrix <- function(predictions, actual) {
  result <- confusionMatrix(table(predictions, actual))
  result$table
}

#(70-30 split)
conf_matrix_gini1 <- conf_matrix(predictions_gini1, testData1$diagnosis)
metrics_gini1 <- calculate_metrics(conf_matrix_gini1)

#(60-40 split)
conf_matrix_gini2 <- conf_matrix(predictions_gini2, testData2$diagnosis)
metrics_gini2 <- calculate_metrics(conf_matrix_gini2)

#(80-20 split)
conf_matrix_gini3 <- conf_matrix(predictions_gini3, testData3$diagnosis)
metrics_gini3 <- calculate_metrics(conf_matrix_gini3)
```

```{r}
# Print accuracies and additional metrics for comparison
print("Evaluation Metrics for Gini Index with (70-30 split):")
print(paste("Accuracy:", accuracy_gini1))
print(paste("Precision:", metrics_gini1[1]))
print(paste("Sensitivity:", metrics_gini1[2]))
print(paste("Specificity:", metrics_gini1[3]))
print("Confusion Matrix:")
print(conf_matrix_gini1)

print("Evaluation Metrics for Gini Index with (60-40 split):")
print(paste("Accuracy:", accuracy_gini2))
print(paste("Precision:", metrics_gini2[1]))
print(paste("Sensitivity:", metrics_gini2[2]))
print(paste("Specificity:", metrics_gini2[3]))
print("Confusion Matrix:")
print(conf_matrix_gini2)

print("Evaluation Metrics for Gini Index with (80-20 split):")
print(paste("Accuracy:", accuracy_gini3))
print(paste("Precision:", metrics_gini3[1]))
print(paste("Sensitivity:", metrics_gini3[2]))
print(paste("Specificity:", metrics_gini3[3]))
print("Confusion Matrix:")
print(conf_matrix_gini3)
```

#### c- Gain Ratio

goal of the code is to employ the J48 algorithm, a Weka implementation of the C4.5 decision tree algorithm, with the Gain Ratio method for selecting attributes. Decision trees are powerful tools for classification, and the Gain Ratio method helps identify the most informative attributes for building an effective model.

#### 1. library of Gain Ratio trees

The RWeka package in R does not have a direct argument to specify the attribute selection measure (such as Information Gain, Information Gain Ratio, or Gini Index), However, we used the J48() function without specifying the attribute selection measure, and it will automatically use Gain Ratio for decision tree construction because Gain Ratio is the default attribute selecting measure.

```{r}
# Gain ratio method for selecting attributes using j48 algorithm
# Load the RWeka library
library(RWeka)

# Set seed for reproducibility
set.seed(7)

# Define the formula for the decision tree
formula_gain_ratio <- diagnosis ~ .

# Fit decision tree models using Gain Ratio
# Build decision tree model using Gain Ratio selected attributes (70-30 split)
model_gain_ratio1 <- J48(formula_gain_ratio, data = trainData1)
# Build decision tree model using Gain Ratio selected attributes (60-40 split)
model_gain_ratio2 <- J48(formula_gain_ratio, data = trainData2)
# Build decision tree model using Gain Ratio selected attributes (80-20 split)
model_gain_ratio3 <- J48(formula_gain_ratio, data = trainData3)
```

#### 2. plot Gain Ratio trees

This code visualize decision trees generated using the Gain Ratio attribute selection method. The code involves plotting the decision trees for three different data splits (70-30, 60-40, and 80-20)

the decision trees plotted using Information Gain and Gain Ratio attribute selection methods appear to be exactly the same, the models might have found similar or identical splits on the datasets and the reasons for this are:

1-Similar Decision Boundaries: Both Information Gain and Gain Ratio methods aim to find optimal decision boundaries based on the attributes in the dataset. The dataset characteristics are very similar attribute splits witch leads to optimal information gain or gain ratio,and the resulting decision trees may be identical.

2- Homogeneous Dataset: The dataset is relatively homogeneous, meaning that the distribution of features and target class is consistent across different partitions, it can lead to similar decision trees.

3- Strong Predictive Features: Certain features are strongly contribute to predicting the target class, both attribute selection methods might identify these features as crucial, leading to similar or identical splits.

4- Random Seed and Reproducibility: The use of the same random seed (as set with set.seed(7)) ensures reproducibility in the model-building process. If the random seed is the same for both models, it can lead to similar splits during the decision tree construction.

5- Evaluation Metrics: While the visualization may appear the same, it's also crucial to assess other performance metrics (accuracy, precision, recall) to confirm if the models are indeed identical in terms of predictive capabilities. Upon confirming the performance metrics,it becomes evident that the models, despite sharing similar visual representations, exhibit significant disparities in their predictive capabilities.

The difference in these crucial metrics underscores the importance of looking beyond the visual similarities and conducting a thorough examination of the underlying model structures to know the nuanced differences influencing their performance.

```{r}
# Plot or print the decision trees
# (70-30 split)
plot(model_gain_ratio1, main = "Decision Tree - Gain Ratio (70-30 split)")
```

```{r}
# (60-40 split)
plot(model_gain_ratio2, main = "Decision Tree - Gain Ratio (60-40 split)")
```

```{r}
# (80-20 split)
plot(model_gain_ratio3, main = "Decision Tree - Gain Ratio (80-20 split)")
```

#### 3. Make predictions of Gain Ratio trees

The decision tree models, constructed based on the Gain Ratio algorithm, are utilized to predict outcomes on three separate test sets. These test sets represent different proportions of the original dataset, with a 70-30 split, a 60-40 split, and an 80-20 split.

```{r}
# Make predictions
# (70-30 split)
predictions_gain_ratio1 <- predict(model_gain_ratio1, testData1)
# (60-40 split)
predictions_gain_ratio2 <- predict(model_gain_ratio2, testData2)
# (80-20 split)
predictions_gain_ratio3 <- predict(model_gain_ratio3, testData3)
```

#### 4. Evaluate the models of Gain Ratio trees

#### 1. Gain Ratio Decision Trees

In this section, we evaluate the predictive performance of decision tree models generated using the Gain Ratio method. The analysis is conducted on three distinct test datasets representing different splits of the original dataset (70-30 split, 60-40 split, and 80-20 split). Performance metrics such as accuracy, precision, sensitivity, and specificity are assessed for each model, providing insights into their efficacy across various data partitions.

#### 2. Methodology

2.1. Accuracy Calculation The accuracy of each model is computed by comparing the predicted values against the actual values in the test set.

2.2. Precision, Sensitivity, Specificity Calculation A custom function, calculate_metrics, is employed to compute precision, sensitivity, and specificity based on the confusion matrix.

2.3. Confusion Matrix Generation The conf_matrix function constructs a confusion matrix using the confusionMatrix function from the caret package.

2.4. Print Metrics for Each Split Metrics such as accuracy, precision, sensitivity, specificity, and the confusion matrix are printed for each split to provide a detailed evaluation.

#### 3. Results

1.  Accuracy (70-30 Split): Approximately 89.08% (60-40 Split): Around 92.06% (80-20 Split): Approximately 91.38%

2.  Precision (70-30 Split): Approximately 87.14% (60-40 Split): Around 86.84% (80-20 Split): Approximately 97.56%

3.  Sensitivity (70-30 Split): Approximately 85.92% (60-40 Split): Around 90.41% (80-20 Split): Approximately 81.63%

4.  Specificity (70-30 Split): Approximately 91.26% (60-40 Split): Around 92.91% (80-20 Split): Approximately 98.51%

Confusion Matrices:

(70-30 Split): True Positives: 61 True Negatives: 94 False Positives: 9 False Negatives: 10

(60-40 Split): True Positives: 66 True Negatives: 131 False Positives: 10 False Negatives: 7

(80-20 Split): True Positives: 39 True Negatives: 68 False Positives: 1 False Negatives: 9

#### 4. Overall Assessment

The decision tree models generated using the Gain Ratio method exhibit varying performance across different data splits. While accuracy is consistently high, other metrics such as precision, sensitivity, and specificity may show fluctuations. The (60-40 Split) configuration, with its higher accuracy, stands out as a strong performer.

Reasons for (60-40 Split) Outperformance:

1- Higher Accuracy: The model achieves the highest accuracy of approximately 92.06% for the 60-40 split, indicating superior predictive capabilities compared to other splits.

2- Balanced Precision and Sensitivity: The precision for positive predictions is notably high at around 86.84%, indicating a high correctness in predicting positive instances. Sensitivity is also robust at approximately 90.41%, demonstrating the model's effectiveness in identifying positive instances.

3- Strong Specificity: Specificity reaches approximately 92.91%, indicating strong correctness in predicting negative instances. This is crucial for minimizing false negatives and ensuring the model's reliability in identifying non-positive cases. While the models perform well across different splits, the (60-40 Split) configuration stands out as the most favorable based on the overall balance of accuracy, precision, sensitivity, and specificity.

```{r}
# Evaluate the models accuracy
# (70-30 split)
accuracy_gain_ratio1 <- sum(predictions_gain_ratio1 == testData1$diagnosis) / nrow(testData1)
# (60-40 split)
accuracy_gain_ratio2 <- sum(predictions_gain_ratio2 == testData2$diagnosis) / nrow(testData2)
# (80-20 split)
accuracy_gain_ratio3 <- sum(predictions_gain_ratio3 == testData3$diagnosis) / nrow(testData3)

# Function to calculate Precision, Sensitivity, Specificity
calculate_metrics <- function(conf_matrix) {
  precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
  sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
  specificity <- conf_matrix[1, 1] / sum(conf_matrix[1, ])
  c(precision, sensitivity, specificity)
}

# Confusion matrix function
conf_matrix <- function(predictions, actual) {
  result <- table(predictions, actual)
  result
}

# (70-30 split)
conf_matrix_gain_ratio1 <- conf_matrix(predictions_gain_ratio1, testData1$diagnosis)
metrics_gain_ratio1 <- calculate_metrics(conf_matrix_gain_ratio1)

# (60-40 split)
conf_matrix_gain_ratio2 <- conf_matrix(predictions_gain_ratio2, testData2$diagnosis)
metrics_gain_ratio2 <- calculate_metrics(conf_matrix_gain_ratio2)

# (80-20 split)
conf_matrix_gain_ratio3 <- conf_matrix(predictions_gain_ratio3, testData3$diagnosis)
metrics_gain_ratio3 <- calculate_metrics(conf_matrix_gain_ratio3)
```

```{r}
# Print accuracies and additional metrics for comparison
print("Evaluation Metrics for Gain Ratio with (70-30 split):")
print(paste("Accuracy:", accuracy_gain_ratio1))
print(paste("Precision:", metrics_gain_ratio1[1]))
print(paste("Sensitivity:", metrics_gain_ratio1[2]))
print(paste("Specificity:", metrics_gain_ratio1[3]))
print("Confusion Matrix:")
print(conf_matrix_gain_ratio1)

print("Evaluation Metrics for Gain Ratio with (60-40 split):")
print(paste("Accuracy:", accuracy_gain_ratio2))
print(paste("Precision:", metrics_gain_ratio2[1]))
print(paste("Sensitivity:", metrics_gain_ratio2[2]))
print(paste("Specificity:", metrics_gain_ratio2[3]))
print("Confusion Matrix:")
print(conf_matrix_gain_ratio2)

print("Evaluation Metrics for Gain Ratio with (80-20 split):")
print(paste("Accuracy:", accuracy_gain_ratio3))
print(paste("Precision:", metrics_gain_ratio3[1]))
print(paste("Sensitivity:", metrics_gain_ratio3[2]))
print(paste("Specificity:", metrics_gain_ratio3[3]))
print("Confusion Matrix:")
print(conf_matrix_gain_ratio3)
```

# 7. CLUSTERING

For clustering we used the K-means method with three different random K sizes, the reason for using K-means as the main clustering methos is K-means clustering yields easily interpretable results that offer valuable insights into the inherent structure of the data. For K sizes 3 random K sizes were picked Randomly selecting K sizes can serve as an exploratory data analysis technique. By trying different K values, we tried to gain insights into the underlying structure of the data and hidden patterns. Random K sizes the were picked are 2,3 and 4. Which will the be evaluated and configured.

```{r}
#CLUSTERING
library(factoextra)
library(ggpubr)
library(cluster)
library(NbClust)

#CLUSTERING
classLabe<- preprocessedData[,22]
#remove the class lables
unlabledData <- preprocessedData[, -22]
print(unlabledData)

unlabledData <- scale(unlabledData)
```

## 1- K-means method with 2 clusters.

By accessing coordinates of a centroid for cluster1 with 2 clusters we come to know that all elements in cluster 2 have higher tumor measurements the the ones in cluster 1, except for the features texture_se and smoothness_se.

```{r}
#k=2
set.seed(123)
Kresult <- kmeans(unlabledData, 2)
summary(Kresult)
```

```{r}
fviz_cluster(Kresult, data = unlabledData)
centroid_coordinates <- Kresult$centers
```

```{r}
print(centroid_coordinates)
```

## 2- K-means method with 3 clusters:

By accessing coordinates of a centroid for cluster2 with 3 clusters we come to know that most elements in cluster 1 and 2 have lower tumor measurements the the ones in cluster3. whilest texture_se and smoothness_se here also were the lowst among cluster3. There is a slight overlapping between cluster which made us discard this number of clusters.

```{r}
#k=3
Kresult1 <- kmeans(unlabledData, 3)
summary(Kresult1)
```

```{r}
fviz_cluster(Kresult1, data = unlabledData)
centroid_coordinates1 <- Kresult1$centers
```

```{r}
print(centroid_coordinates1)
```

## 3- K-means method with 4 clusters:

For the last clustering with 4 clusters the pattern was pretty similar to the previous ones clusters 3 and 4 have higher measurements except for the features such as smoothness_se and texture_se. However there is an obvious overlapping between clusters which made us discard this size of clustering.

```{r}
#k=4
Kresult2 <- kmeans(unlabledData, 4)
summary(Kresult2)
```

```{r}
fviz_cluster(Kresult2, data = unlabledData)
centroid_coordinates2 <- Kresult2$centers
```

```{r}
print(centroid_coordinates2)
```

### Overall Assessmentin

conclusion we notice a pattern from the clusters that with high measurements of the rest of the features come low measurements of smoothness_se and texture_se and vise versa

## 4- Evaluation of clustering:

#### Silhouette method:

average silhouette plots that provide insights into the quality of clustering for different values of k (number of clusters). when analyzing these average silhouette plots, you should look for high average silhouette widths and well-separated distributions of silhouette widths. Higher average silhouette widths indicate better clustering, while well-separated distributions suggest clear boundaries between clusters. These characteristics imply that the chosen number of clusters provides meaningful and distinct groups within the data.

##### Average silhouette plot with k=2:

```{r}
###Within-cluster sum of squares 
wss <- Kresult$tot.withinss
print(wss)
# Bcubed and recall dor each clustur
cluster_assignments <- c(Kresult$cluster)
ground_truth_labels <- c(classLabe$diagnosis )

if (length(cluster_assignments) > length(ground_truth_labels)) {
  cluster_assignments <- cluster_assignments[1:length(ground_truth_labels)]
} else if (length(cluster_assignments) < length(ground_truth_labels)) {
  ground_truth_labels <- ground_truth_labels[1:length(cluster_assignments)]
}

data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)
```

```{r}
# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0
  
  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
    # Count the number of items from the same category within the same cluster
    same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
    # Count the total number of items in the same cluster
    total_same_cluster <- sum(data$cluster == cluster)
    
    # Count the total number of items with the same category
    total_same_category <- sum(data$label == label)
    
    # Calculate precision and recall for the current item and add them to the sums
    precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n
  
  return(list(precision = precision, recall = recall))
}
```

```{r}
# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)

# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall

# Print the results
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:",recall,"\n")
```

```{r}
#average silhouette for each clusters 
avg_sil <- silhouette(Kresult$cluster,dist(unlabledData)) #a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
fviz_silhouette(avg_sil)#k-means clustering with estimating k and initializations
```

#### Average silhouette plot with k=3:

```{r}
###Within-cluster sum of squares 
wss <- Kresult1$tot.withinss
print(wss)
```

```{r}
# Bcubed and recall dor each clustur
cluster_assignments <- c(Kresult1$cluster)
ground_truth_labels <- c(classLabe$diagnosis )

if (length(cluster_assignments) > length(ground_truth_labels)) {
  cluster_assignments <- cluster_assignments[1:length(ground_truth_labels)]
} else if (length(cluster_assignments) < length(ground_truth_labels)) {
  ground_truth_labels <- ground_truth_labels[1:length(cluster_assignments)]
}

data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)
```

```{r}
# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0
  
  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
    # Count the number of items from the same category within the same cluster
    same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
    # Count the total number of items in the same cluster
    total_same_cluster <- sum(data$cluster == cluster)
    
    # Count the total number of items with the same category
    total_same_category <- sum(data$label == label)
    
    # Calculate precision and recall for the current item and add them to the sums
    precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n
  
  return(list(precision = precision, recall = recall))
}
```

```{r}
# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)

# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall

# Print the results
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:",recall,"\n")
```

```{r}
#average silhouette for each clusters 
avg_sil <- silhouette(Kresult1$cluster,dist(unlabledData)) #a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
fviz_silhouette(avg_sil)#k-means clustering with estimating k and initializations
```

#### Average silhouette plot with k=4:

```{r}
###Within-cluster sum of squares 
wss <- Kresult2$tot.withinss
print(wss)
```

```{r}
# Bcubed and recall dor each clustur
cluster_assignments <- c(Kresult2$cluster)
ground_truth_labels <- c(classLabe$diagnosis )

if (length(cluster_assignments) > length(ground_truth_labels)) {
  cluster_assignments <- cluster_assignments[1:length(ground_truth_labels)]
} else if (length(cluster_assignments) < length(ground_truth_labels)) {
  ground_truth_labels <- ground_truth_labels[1:length(cluster_assignments)]
}

data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)
```

```{r}
# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0
  
  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
    # Count the number of items from the same category within the same cluster
    same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
    # Count the total number of items in the same cluster
    total_same_cluster <- sum(data$cluster == cluster)
    
    # Count the total number of items with the same category
    total_same_category <- sum(data$label == label)
    
    # Calculate precision and recall for the current item and add them to the sums
    precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n
  
  return(list(precision = precision, recall = recall))
}
```

```{r}
# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)

# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall

# Print the results
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:",recall,"\n")
```

```{r}
#average silhouette for each clusters 
avg_sil <- silhouette(Kresult2$cluster,dist(unlabledData)) #a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
fviz_silhouette(avg_sil)#k-means clustering with estimating k and initializations
```

## 5- Optimal number of Clusters:

fvis_Nbclust-Cluster Validation: \### a- Silhouette method Using the Silhouette method the Optimal number of clusters is 2

```{r}
fviz_nbclust(unlabledData, kmeans, method = "silhouette")+ labs(subtitle = "Silhouette method")
```

### b- Majority rule

According to the majority rule, the best number of clusters is 2

```{r}
fres.nbclust <- NbClust(unlabledData, distance="euclidean", min.nc = 2, max.nc = 15, method="kmeans", index="all")
```

### c- elbow method

we noticed that the best elbow point is 3

```{r}
fviz_nbclust(unlabledData, kmeans, method = "wss") +
  geom_vline(xintercept = 3, linetype = 2)+
  labs(subtitle ="Elbow method")
```

### Overall Assessmentin

In conclusion and based on the majority rule we believe that the best number of clusters for this data set is 2.

```{r}
#Clustering center visualization
plot(unlabledData[, "compactness_mean"], col = Kresult$cluster)
points(Kresult$centers[, "compactness_mean"], col = 1:4, pch = 8, cex = 2) # plot cluster centers
```

# 8. Evaluation and Comparison

## a- Classification

| Model            | Data Split  | Accuracy | Precision | Sensitivity | Specificity |
|------------------|-------------|----------|-----------|-------------|-------------|
| Information Gain | 70-30 Split | 89.66%   | 88.57%    | 86.11%      | 92.16%      |
| Information Gain | 60-40 Split | 93.93%   | 92.11%    | 90.91%      | 95.62%      |
| Information Gain | 80-20 Split | 92.24%   | 95.12%    | 84.78%      | 97.14%      |
| Gini Index       | 70-30 Split | 88.51%   | 77.14%    | 93.10%      | 86.21%      |
| Gini Index       | 60-40 Split | 90.19%   | 92.11%    | 82.35%      | 95.35%      |
| Gini Index       | 80-20 Split | 93.97%   | 95.12%    | 88.64%      | 97.22%      |
| Gain Ratio       | 70-30 Split | 89.08%   | 86.21%    | 91.55%      | 87.36%      |
| Gain Ratio       | 60-40 Split | 91.38%   | 97.56%    | 81.63%      | 98.51%      |
| Gain Ratio       | 80-20 Split | 88.08%   | 81.82%    | 89.47%      | 85.71%      |

In the classification task, different evaluation metrics and criteria are used to assess the performance of models like accuracy, precision, sensitivity, and specificity for models using different data splits and different splitting criteria (Information Gain, Gini Index, and Gain Ratio).

analyzing the results: the Gini Index with an 80-20 data split is considered the best based on the provided metrics.

Gini Index - 80-20 Split: Accuracy: 93.97% Precision: 95.12% Sensitivity (Recall): 88.64% Specificity: 97.22%

This model has a high accuracy, precision, and specificity, indicating that it performs well in correctly classifying instances of both classes. The sensitivity is also relatively high, suggesting a good ability to identify positive instances.Especially in a binary classification problems like ours, where the classes may have imbalanced distributions, a high specificity is often desirable.And the 80-20 split seems to perform well for the Gini Index.

## b- CLUSTERING

|                                    | K=2       | K=3       | K=4       |
|------------------------------------|-----------|-----------|-----------|
| Average Silhouette width           | 0.39      | 0.34      | 0.31      |
| Total within-cluster sum of square | 7121.733  | 6059.179  | 5670.971  |
| Bcubed percision                   | 0.8865035 | 0.8552597 | 0.8575291 |
| Bcubed recall                      | 0.8940466 | 0.699343  | 0.657348  |

In the clustering task, different metrics are used to evaluate the quality of clustering. The provided metrics include Average Silhouette Width, Total Within-Cluster Sum of Squares, Bcubed Precision, and Bcubed Recall for different values of k (number of clusters).

analyzing the results:K=2 seems to be the best model. It has the highest average silhouette width, indicating well-separated clusters, and the highest precision and recall, suggesting better internal cluster quality.

Average Silhouette Width: A higher silhouette width indicates better-defined clusters. In this case, K=2 has the highest silhouette width, suggesting well-separated clusters.

Total Within-Cluster Sum of Squares: A lower sum of squares indicates tighter clusters. K=2 has the highest value, indicating that instances within clusters are closer to each other compared to other values of k.

Bcubed Precision and Recall: Higher precision and recall values indicate better cluster quality. K=2 has the highest precision and recall, suggesting that the clusters are more internally consistent and the algorithm is better at finding true positive instances.

K=2: Average Silhouette Width: 0.39 Total Within-Cluster Sum of Squares: 7121.733 Bcubed Precision: 0.8865035 Bcubed Recall: 0.8940466
