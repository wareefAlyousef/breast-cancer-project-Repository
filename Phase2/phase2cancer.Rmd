---
title: "phase2cancer"
output: html_notebook
date: "2023-10-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Problem

The problem is the accurate diagnosis of breast masses as malignant (cancerous) or benign (non- cancerous) using computed features from digitized images of fine needle aspirate (FNA). The features consist of radius, texture, perimeter, area, smoothness, compactness, concavity, concave.points, symmetry, and fractal_dimension of the mass,and there mean, worst, and standard error(SE).

The goal is to predict the nature of breast tumors based on the analyzed features, enabling medical professionals to make informed decisions regarding patient treatment and care.

Solving the problem of diagnosing breast cancer is important because it can lead to early detection, appropriate treatment plans, improved survival rates, and optimal allocation of healthcare resources.

# 2. Data Mining Task

The problem can be defined as a data mining binary classification task. The task involves training models and algorithms on the features of breast masses to predict whether a specific mass is malignant(M) or benign(B). The objective is to optimize the classification model's accuracy and performance in differentiating between the two classes using the dataset as the training data. Data mining techniques such as feature selection, model training, and performance evaluation would be employed to solve this task efficiently.

# 3. Data

We will go over a detailed description of the dataset used in this project. The dataset in focus is the Breast Cancer Wisconsin (Diagnostic) dataset, which plays a crucial role in achieving the goal of accurate breast cancer diagnosis and classification.

### The source of the dataset

<https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data>

### General information about the dataset

1- Number of attributes (variables, or columns): 32 (the last column (X) Contains only missing "NA" values, that why we didn't count it, and it well be removed later on)

2- Number of objects (records, or rows): The dataset contains a total of 569 instances, each representing a sample of a breast mass.

3- all variables are numeric, except id, and diagnosis.

4- The ID number is a nominal attribute

5- diagnosis is a binary (categoric, or factor) attribute and has two levels of values "B","M".

6- Class name or labels: The diagnosis attribute represents the class labels, with "M" indicating malignant (cancerous) tumors and "B" indicating benign (non-cancerous) tumors.

7- Class distribution: The dataset is imbalanced, with 357 instances classified as benign and 212 instances classified as malignant.

### Attributes table

```         
The attributes table present important information about the attributes associated with the dataset. This table serves as a reference guide, outlining the different attributes, their description, data type, and possible values. It aims to provide a comprehensive overview of the dataset, enabling us to understand and utilize the information effectively.
```

| No  | Attributes              | Description                                                                                                                                                      | Data type                      | Possible values                  |
|-------------|-------------|--------------------|-------------|-------------|
| 1   | id                      | Id number for applicant                                                                                                                                          | nominal                        | Range between 8670-911320502     |
| 2   | diagnosis               | indicate whether a case is classified as malignant (cancerous) or benign.                                                                                        | (Asymmetric Binary)categorical | "M" for malignant "B" for benign |
| 3   | radius_mean             | Radius: is a measurement of the average distance from the center of the nucleus to its boundary. Mean: mean of distances from center to points on the perimeter. | numerical                      | Range between 6.9-28.1           |
| 4   | texture_mean            | standard deviation of gray-scale values.                                                                                                                         | numerical                      | Range between 9.7-39.2           |
| 5   | perimeter_mean          | Perimeter: total length of the boundary of the nucleus. Mean: mean size of the core tumor                                                                        | numerical                      | Range between 43.7-188.5         |
| 6   | area_mean               | The mean value of total area occupied by the nucleus.                                                                                                            | numerical                      | Range between 143.5-782.7        |
| 7   | smoothness_mean         | local variation in the radius lengths of the cell nuclei present in a breast mass. It quantifies the smoothness of the boundaries of the nuclei.                 | numerical                      | Range between 0.05-o.16          |
| 8   | compactness_mean        | measures the smoothness of the boundaries and relates to the compactness of the shape of the nuclei. mean of perimeter\^2/ area - 1.0                            | numerical                      | Range between 0.01-0.34          |
| 9   | concavity_mean          | mean of severity of concave portions of the contour                                                                                                              | numerical                      | Range between 0.0-0.4            |
| 10  | concave.points_mean     | mean for number of concave portions of the contour                                                                                                               | numerical                      | Range between 0.0-0.2            |
| 11  | symmetry_mean           | Mean quantifies the degree to which the shape of the nuclei is symmetrical.                                                                                      | numerical                      | Range between 0.1-0.3            |
| 12  | fractal_dimension_mean  | quantifies the complexity and irregularity of the nuclei's shape using fractal geometry. mean for "coastline approximation" - 1                                  | numerical                      | Range between 0.04-0.09          |
| 13  | radius_se               | standard error for the mean of distances from center to points on the perimeter                                                                                  | numerical                      | Range between 0.1-2.8            |
| 14  | texture_se              | standard error for standard deviation of gray-scale values                                                                                                       | numerical                      | Range between 0.3-4.8            |
| 15  | perimeter_se            | Standard error for mean size of the core tumor                                                                                                                   | numerical                      | Range between 0.7-21.9           |
| 16  | area_se                 | Standard error for The mean value of total area occupied by the nucleus.                                                                                         | numerical                      | Range between 6.8-542.2          |
| 17  | smoothness_se           | standard error for local variation in radius lengths                                                                                                             | numerical                      | Range between 0.001-0.0311       |
| 18  | compactness_se          | standard error for perimeter\^2 / area -1.0                                                                                                                      | numerical                      | Range between 0.002-0.135        |
| 19  | concavity_se            | standard error for severity of concave portions of the contour                                                                                                   | numerical                      | Range between 0.00-0.39          |
| 20  | concave.points_se       | standard error for number of concave portions of the contour                                                                                                     | numerical                      | Range between 0.00-0.05          |
| 21  | symmetry_se             | Standard error for the mean that quantifies the degree to which the shape of the nuclei is symmetrical.                                                          | numerical                      | Range between 0.007-0.078        |
| 22  | fractal_dimension_se    | standard error for "coastline approximation" - 1                                                                                                                 | numerical                      | Range between 0.000-0.029        |
| 23  | radius_worst            | "worst" or largest mean value for mean of distances from center to points on the perimeter                                                                       | numerical                      | Range between 7.93-36.04         |
| 24  | texture_worst           | "worst" or largest mean value for standard deviation of gray-scale values                                                                                        | numerical                      | Range between 12.02-49.54        |
| 25  | perimeter_worst         | "worst" or largest mean value for the size of the core tumor                                                                                                     | numerical                      | Range between 50.41-251.20       |
| 26  | area_worst              | "worst" or largest mean value for total area occupied by the nucleus.                                                                                            | numerical                      | Range between 185.2-4254.0       |
| 27  | smoothness_worst        | "worst" or largest mean value for local variation in radius lengths                                                                                              | numerical                      | Range between 0.07-0.22          |
| 28  | compactness_worst       | "worst" or largest mean value for perimeter\^2 / area - 1.0                                                                                                      | numerical                      | Range between 0.02-1.05          |
| 29  | concavity_worst         | "worst" or largest mean value for severity of concave portions of the contour                                                                                    | numerical                      | Range between 0.00-1.2           |
| 30  | concave.points_worst    | "worst" or largest mean value for number of concave portions of the contour                                                                                      | numerical                      | Range between 0.00-0.29          |
| 31  | symmetry_worst          | "worst" or largest mean value that quantifies the degree to which the shape of the nuclei is symmetrical.                                                        | numerical                      | Range between 0.1-0.6            |
| 32  | fractal_dimension_worst | "worst" or largest mean value for "coastline approximation" - 1                                                                                                  | numerical                      | Range between 0.05-0.20          |
| 33  | X                       | has no value all records are missing "NA"                                                                                                                        | logical                        | "NA"                             |

# 4.Data preprocessing

Data preprocessing is a vital step in data mining, it involves transforming raw data into a format that is suitable for modeling. The process of data preprocessing contains various techniques and methods to clean, transform, and prepare data before it can be utilized for training models or extracting meaningful insights.

### 1- Data cleaning

Data cleaning involves identifying and correcting or removing inconsistencies, errors, and inaccuracies in the dataset. Missing values need to be handled appropriately to avoid bias or distortion in the analysis. Outliers, which are extreme values that deviate significantly from the majority of the data, can impact the accuracy of models and need to be detected and addressed.

Fortunately, we don't have any missing values in our dataset, but we were able to identify a total of 265 outliers using the z-score method. The number of outliers for each attribute ranged between 65 and 0, with area_se having the most outliers, and texture_worst, smoothness_worst, and concave points_worst having the fewest outliers.

We did not remove all the outlier objects because there were over 100 of them and removing all of them could compromise the data's integrity. To settle the issue, we decided to remove the most frequent outlier objects.

To know which objects were the most frequent outliers, we counted the number of times an object became an outlier to a certain attribute, it ranged between 11 to 0, with object number 462 being the most frequent outlier, then we decided that every object that was an outlier more than five times will be deleted, therefore we deleted a total of 13 object.

### 2- Data reduction

We had an attribute in our dataset that was entirely made up of null values, thus we had to eliminate it under dimensionality reduction, attribute subset reduction, because it was an irrelevant attribute that contained no useful information.

All the other attributes were relevant to the class label where they describe the mass such as its radius, area or the texture, but the id attribute is not relevant to the class label since it does not describe the mass in any sense, so we had to perform dimensionality reduction that falls under attribute subset reduction, since the id didn't contain any useful information.

### 3- Attribute transformation

After data reduction, all the remaining attributes were numeric except for the diagnosis, which was nominal, precisely categorical Asymmetric Binary, where M stands for malignant and B stands for benign, so we had to perform an attribute transformation on the diagnosis in which we encoded every B to become 0, and every M to become 1.

### 4- Feature scaling

The scale() function is utilized to calculate the mean and standard deviation of the selected columns (1 to 30). Each element in the columns is then scaled by subtracting the mean and dividing by the standard deviation. Feature scaling is essential as it brings the features to a standardized range, preventing any single feature from dominating the learning process due to differences in scale.

### 5- Normalization

Normalization is a preprocessing technique that scales numerical features to a consistent range, typically between 0 and 1. By applying the normalize() function to the selected columns, the code ensures that no single feature dominates the learning process due to differences in scale. This normalization step is valuable for improving the performance and stability of data mining algorithms.

### 6- Discretization

Based on the selected data set we decided to use equal width in discretization process , performed on the selected columns. We selected all numeric countinues attributes (In this case, all columns except the last one were selected), and divided using equal width with 20 bins.

The discretization process involved applying the equal_width_discretize() function to each column selected using lapply(). The resulting discrete values were then mapped back to the data set, data1, and displayed using View().

The reason for choosing an equal-width distribution in this case is to divide the range of values in each selected column into a fixed number of equal-sized bins. This provides us with several features that help us in analysis:

1- Simplicity and interpretability: Equal width divides the range of values into equal-sized intervals, making the resulting categories easier to interpret.

2- Preserving the distribution: By dividing the range into equal intervals, it ensures that each bin covers an equal width of the data range, allowing for a more balanced representation of values.

### 7- Feature selection

We performed a feature selection by assessing the importance of attributes in the dataset. The code trains a linear regression model and calculates the importance of each attribute. The least important 9 attributes were identified by their importance being under 0.5, these attributes were symmetry_se, fractal_dimension_mean, texture_mean, concave points_se, perimeter_se, symmetry_mean, compactness_se, smoothness_mean, and compactness_worst, while the highest importance being 3.05 and belonging to compactness_mean. The least important 9 attributes are subsequently recommended for removal, removing the least important 9 attributes can be beneficial for several reasons:

Dimensionality reduction: Removing less important attributes reduces the dimensionality of the dataset, making it more manageable. It simplifies the model and can lead to improved model performance.

Computational efficiency: With fewer attributes, the training and evaluation of the model become faster and more efficient. Removing unnecessary attributes reduces the computational cost associated with processing and analyzing the data.

Simplifying the model: By focusing on the most important attributes, the model becomes more interpretable and easier to understand. It allows for better insights into the relationships between the selected attributes and the target variable.

# 5. Code

```{r}
library("tidyverse")
library("dplyr")
library("readr")
library("Hmisc")
library("outliers")
library("mlbench")
library("ggplot2")
library("lattice")
library("caret")
library("caTools")
library("corrplot")
library("reshape2")
```

```{r}
data <- read_csv("C:/Users/warif/OneDrive/Desktop/mining project data/data.csv")
head(data)
```

```{r}
nrow(data)
```

```{r}
ncol(data)
```

```{r}
dim(data)
```

```{r}
names(data)
```

```{r}
str(data)
```

```{r}
summary(data)
```

```{r}
describe(data)
```

```{r}
data1 <- data
head(data1)
```

## Data reduction

```{r}
data1 <- subset(data1, select = -...33)
data1 <- subset(data1, select = -id)
data1 <- data1[, c(2:ncol(data1), 1)]
head(data1)
```

## frequency table

```{r pressure, echo=FALSE}
diagnosis.table <- table(data$diagnosis)
colors <- terrain.colors(2) 
# a pie chart for the diagnosis 
diagnosis.prop.table <- prop.table(diagnosis.table)*100
diagnosis.prop.df <- as.data.frame(diagnosis.prop.table)
pielabels <- sprintf("%s - %3.1f%s", diagnosis.prop.df[,1], diagnosis.prop.table, "%")
colors = c ("pink", "skyblue1")
pie(diagnosis.prop.table,
    labels=pielabels,  
    clockwise=TRUE,
    col=colors,
    border="gainsboro",
    radius=0.8,
    cex=0.8, 
    main="Frequency Of Cancer Diagnosis")
legend(1, .4, legend=diagnosis.prop.df[,1], cex = 0.7,fill = colors)
```

```{r}
#Remove the first  column
data2 <- data[, -1]
#Remove the last column
data2 <- data2[,-32]
#Tidy the data
#bc_data$diagnosis <- as.factor(bc_data$diagnosis)
summary(data2)
```

```{r}
head(data2)
```

## corlrrelation

```{r}
#corlrrelation
c <- cor(data2[,2:31])
corrplot(c, order = "hclust", tl.cex = 0.7)
```

## covariance

```{r}
#covariance 
cov(data2[, 2:31])
```

## BoxPlot

```{r}
# Comparing the raidus, area and concavity of benging malignant stage
ggplot(data2, aes(x=diagnosis, y=radius_mean))+geom_boxplot()+ggtitle("area of bengin Vs malignant")
```

```{r}
ggplot(data2, aes(x=diagnosis, y=concavity_mean))+geom_boxplot()+ggtitle("concavity of bengin Vs malignant")
```

```{r}
# Reshape the data
data_long <- data2 %>%
  pivot_longer(cols = -diagnosis, names_to = "variable", values_to = "value")

# Plot box plots with separate y-axis scales and arranged in a grid
ggplot(data_long, aes(x = diagnosis, y = value)) +
  geom_boxplot() +
  facet_wrap(~ variable, nrow = 5, ncol = 6, scales = "free_y") +
  ggtitle("Box Plot for All Columns with Diagnosis") 
```

## histograms Plot

```{r}
#Break up columns into groups, according to their suffix designation 
#(_mean, _se,and __worst) to perform visualisation plots off.
data_mean <- data[ ,c("diagnosis", "radius_mean", "texture_mean","perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave points_mean", "symmetry_mean", "fractal_dimension_mean" )]

data_se <- data[ ,c("diagnosis", "radius_se", "texture_se","perimeter_se", "area_se", "smoothness_se", "compactness_se", "concavity_se", "concave points_se", "symmetry_se", "fractal_dimension_se" )]

data_worst <- data[ ,c("diagnosis", "radius_worst", "texture_worst","perimeter_worst", "area_worst", "smoothness_worst", "compactness_worst", "concavity_worst", "concave points_worst", "symmetry_worst", "fractal_dimension_worst" )]
```

```{r}
#Plot histograms of "_mean" variables group by diagnosis
ggplot(data = melt(data_mean, id.var = "diagnosis"), mapping = aes(x = value)) + 
  geom_histogram(bins = 10, aes(fill=diagnosis), alpha=0.5) + facet_wrap(~variable, scales =      'free_x')
```

```{r}
#Plot histograms of "_se" variables group by diagnosis
ggplot(data = melt(data_se, id.var = "diagnosis"), mapping = aes(x = value)) + 
  geom_histogram(bins = 10, aes(fill=diagnosis), alpha=0.5) + facet_wrap(~variable, scales = 'free_x')
```

```{r}
#Plot histograms of "_worst" variables group by diagnosis
ggplot(data = melt(data_worst, id.var = "diagnosis"), mapping = aes(x = value)) + 
  geom_histogram(bins = 10, aes(fill=diagnosis), alpha=0.5) + facet_wrap(~variable, scales = 'free_x')
```

# Data preprocessing

## Data cleaning

```{r}
#Checking NULL, FALSE means no null, TRUE cells means the value of the cell is null
is.na(data1)
```

```{r}
# to find the total null values in the dataset
sum(is.na(data1))
```

### Outliers

```{r}
data2 <- subset(data1, select = -diagnosis)
# Function to identify outliers using z-score
find_outliers <- function(column) {
  median_val <- median(column)
  mad_val <- mad(column)
  modified_z_scores <- 0.6745 * (column - median_val) / mad_val
  return(which(abs(modified_z_scores) > 3))
}
# Function to count outliers for each attribute
count_outliers <- function(data) {
  outlier_counts <- sapply(data, function(col) length(find_outliers(col)))
  return(outlier_counts)
}
# Apply the count_outliers function to the data frame
outlier_counts <- count_outliers(data2)
# Print the outlier counts for each attribute
cat("Outlier counts for each attribute:\n")
```

```{r}
print(outlier_counts)
```

```{r}
# Create an empty list to store outlier indices for each attribute
outlier_indices <- list()
# Apply the find_outliers function to each attribute in the data frame
for (col in names(data2)) {
  outlier_indices[[col]] <- find_outliers(data2[[col]])
}
# Print the outlier indices
print(outlier_indices)
```

```{r}
# Flatten the outlier indices into a single vector
flattened_indices <- unlist(outlier_indices)
# Get the outlier counts
outlier_counts <- table(flattened_indices)
# Print the outlier counts
cat("Outlier counts:\n")
```

```{r}
print(outlier_counts)
```

```{r}
# Get the total count of outliers
total_outliers <- sum(outlier_counts)
# Print the total count of outliers
cat("Total count of outliers:", total_outliers, "\n")
```

```{r}
# Get the most frequent outlier objects 
most_frequent_outlier_objects <- if (length(flattened_indices) > 0) {
  table(flattened_indices)
} else {
  NULL
}
# Sort the most frequent outlier objects in descending order
sorted_most_frequent_outliers <- sort(most_frequent_outlier_objects, decreasing = TRUE)
# Get the outlier objects with a frequency greater than 5
frequent_outliers <- sorted_most_frequent_outliers[sorted_most_frequent_outliers > 5]
# Print the outlier objects with their frequencies
cat("Outlier objects with a frequency greater than 5:\n")
```

```{r}
for (row in names(frequent_outliers)) {
  cat("Object", row, "- Frequency:", frequent_outliers[row], "\n")
}
```

```{r}
# Identify the objects to be deleted
objects_to_delete <- as.integer(names(frequent_outliers))
# Delete the objects from the data frame
data1 <- data1[-objects_to_delete, ]
```

## Attribute transformation

```{r}
data1$diagnosis <- ifelse(data1$diagnosis == "M", 1, ifelse(data1$diagnosis == "B", 0, data1$diagnosis))
# Convert the target variable to numeric or logical
data1$diagnosis <- as.numeric(data1$diagnosis)
head(data1)
```

## Feature scaling

```{r}
#scale, with default settings, will calculate the mean and standard deviation of the entire vector, then "scale" each element by those values by subtracting the mean and dividing by the sd
data1 [, 1:30] = scale(data1 [, 1:30])
head(data1)
```

## Normalization

```{r}
# Define function normalize()
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}
cols_to_normalize <- 1:(ncol(data1) - 1)  # Select all columns except the last one
data1[, cols_to_normalize] <- lapply(data1[, cols_to_normalize], normalize)
head(data1)
```

## Discretization

```{r}
# Number of bins for discretization
num_bins <- 20
# Function to perform discretization using equal width
equal_width_discretize <- function(var, num_bins) {
  breaks <- seq(min(var, na.rm = TRUE), max(var, na.rm = TRUE), length.out = num_bins + 1)
  cut(var, breaks = breaks, labels = FALSE)
}
# Subset the dataset with selected columns
cols_to_Discretiz <- 1:(ncol(data1) - 1)  # Select all columns except the last one
# Apply equal width discretization to selected columns
data1_discretized <- lapply(data1[, cols_to_Discretiz], equal_width_discretize, num_bins)
# Replace null values with 0
data1_discretized <- lapply(data1_discretized, function(x) ifelse(is.na(x), 0, x))
# Assign discretized values back to the original dataset
data1[, cols_to_Discretiz] <- data1_discretized
# View the modified dataset
head(data1)
```

## Feature selection

```{r}
# ensure results are repeatable
set.seed(7)
# prepare training scheme
control4 <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
# train the model
model4 <- train(diagnosis ~.,data = data1 , method="lm", preProcess="scale", trControl=control4)
# estimate variable importance
importance4 <- varImp(model4 , scale=FALSE)
# plot importance
plot(importance4)
```

```{r}
# train the model
model4 <- lm(diagnosis ~.,data = data1)
# estimate variable importance
importance4 <- varImp(model4 , scale=FALSE)
# Copy the importance3 dataframe to a new dataframe
importance4_copy <- importance4
# Add a column for row names
importance4_copy$name <- row.names(importance4_copy)
# Sort the dataframe based on the "Overall" column in descending order
importance4_copy <- importance4_copy[order(importance4_copy$Overall, decreasing = TRUE), ]
# Reset row names
row.names(importance4_copy) <- NULL
# Print the rearranged dataframe
print(importance4_copy)
```

```{r}
# Remove the least important 9 attributes
data1 <- subset(data1, select = -symmetry_se)
data1 <- subset(data1, select = -fractal_dimension_mean)
data1 <- subset(data1, select = -texture_mean)
data1 <- subset(data1, select = -`concave points_se`)
data1 <- subset(data1, select = -perimeter_se)
data1 <- subset(data1, select = -symmetry_mean)
data1 <- subset(data1, select = -compactness_se)
data1 <- subset(data1, select = -smoothness_mean)
data1 <- subset(data1, select = -compactness_worst)
head(data1)
```

## Data before preprocessing

```{r}
#Data before preprocessing
rawData <- data
head(rawData)
```

## Preprocessed data

```{r}
#Preprocessed data
preprocessedData <- data1
head(preprocessedData)
```
